# kaggle-titanic-advanced-solution
kaggle titanic advanced solution 

If you are looking for an advanced solution to the Titanic problem, you definitely don't need the basic explanations like the previous repositories. So let's get to the point.

But if you need to, you can also check the previous repositories. I also did this problem with machine learning and deep learning models.



# to start

You should know that we are facing two problems in this issue.

The first is that we have very short data.

The second
is that the surviving and dead labels are not balanced at all. For example, in a data set with 600 rows, only 150-200 people survived.
So you have to be careful about the metric you use to measure your model. I have not seen other repositories that have paid attention to this point. Most of them pay attention to accuracy. But it must have happened to you that you designed a model that had excellent accuracy, but when you submit it, Kaggle gives you a low score. The reason for this is that Kaggle's site measures with an F1 score. That is completely logical because we are dealing with data that is completely unbalanced.

## How did I solve these two problems?

Because I have little data, I used one of the data generation methods.
We have good options for data generation. I used several of them.
But in the end, GAN gave me the best result.

### what is GAN ?

GAN stands for Generative Adversarial Network. It is a type of neural network architecture designed to generate new, synthetic data that resembles a given dataset. GANs were introduced by Ian Goodfellow and his collaborators in 2014 and have since become one of the most popular methods for generative modeling.

Key Components of a GAN:
A GAN consists of two neural networks that work together and compete against each other:

Generator:

The generator creates synthetic data (e.g., images, text, etc.) from random noise.
Its goal is to generate data that is indistinguishable from real data.
Discriminator:

The discriminator evaluates data and determines whether it is real (from the actual dataset) or fake (generated by the generator).
Its goal is to correctly classify inputs as real or fake.

### For model 

I used transformer models. I'll give you a little explanation below.

A Transformer model is a type of neural network architecture introduced in the 2017 paper "Attention is All You Need" by Vaswani et al. It has since become the foundation of many state-of-the-art models in natural language processing (NLP), computer vision, and other domains. The key innovation of the Transformer model is its use of the self-attention mechanism, which allows it to process input sequences efficiently and capture long-range dependencies.












#  Built With


Python - Programming language

Jupyter Notebook - Web application for creating and sharing documents that contain live code, equations, visualizations and narrative text

Pandas - Data analysis and manipulation tool

NumPy - Library for working with arrays

Matplotlib - Library for creating static, animated, and interactive visualizations

Seaborn - Data visualization library based on matplotlib

tensorflow  - deep learning library for the Python programming language

